from llama.tokenizer import Tokenizer
from llama.model import ModelArgs, Llama
import torch
import random
import json

def inference():
    torch.manual_seed(1)

    tokenizer_path = "/project/saifhash_1190/llama2-7b/tokenizer.model"
    model_path = "/home1/garudaia/main_proj/llama_finetune_proj_300_full_ffn.pth"
    test_data_path = "/home1/garudaia/main_proj/alpaca_testset.json"

    tokenizer = Tokenizer(tokenizer_path)

    checkpoint = torch.load(model_path, map_location="cpu")
    model_args = ModelArgs()
    torch.set_default_tensor_type(torch.cuda.HalfTensor)  # load model in fp16
    model = Llama(model_args)
    model.load_state_dict(checkpoint, strict=False)
    model.to("cuda")

    PROMPT_template = {
        "prompt_input": (
            f"""Below is an instruction that describes a task, paired with an input that provides further context. 
                Write a response that appropriately completes the request.\n\n"
                ### Instruction:\n[instruction]\n\n### Input:\n[input]\n\n### Response:"""
        ),
        "prompt_no_input": (
            f"""Below is an instruction that describes a task. 
               Write a response that appropriately completes the request.\n\n"
               ### Instruction:\n[instruction]\n\n### Response:"""
        )
    }

    with open(test_data_path, "r") as f:
        test_data = json.load(f)

    prompts = []
    prompt_print = []

    for i in range(5):
        # indx = random.randint(0, 99)
        indx = random.sample(range(0, 100), 1)[0]
        prompt = test_data[indx]
        if (prompt['input'] == ""):
            add_prompt = PROMPT_template['prompt_no_input'].replace("[instruction]", prompt['instruction'])
        else:
            add_prompt = PROMPT_template['prompt_input'].replace("[instruction]", prompt['instruction'])
            add_prompt = add_prompt.replace("[input]", prompt['input'])

        prompts.append(add_prompt)
        prompt_print.append(prompt)




    model.eval()
    results = model.generate(tokenizer, prompts, max_gen_len=64, temperature=0.6, top_p=0.9)
    i=0
    for prompt, result in zip(prompts, results):
        print(prompt)
        print(f"The response according to dataset - \n {prompt_print[i]['output']}\n\n")
        print(f">Result generated by trained model - \n {result['generation']}\n")
        print("\n==================================\n")
        i += 1


if __name__ == "__main__":
    inference()